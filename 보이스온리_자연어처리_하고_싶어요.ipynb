{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZs7NKo4TIgJEEpV41DDzB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amazingbutuseless/wxnwxxmxngx/blob/main/%EB%B3%B4%EC%9D%B4%EC%8A%A4%EC%98%A8%EB%A6%AC_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%ED%95%98%EA%B3%A0_%EC%8B%B6%EC%96%B4%EC%9A%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yvuVERzUC6U-",
        "outputId": "62c3f3d8-a788-47e5-fb75-c282568cd2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install nltk konlpy pandas scikit-learn matplotlib wordcloud networkx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트\n",
        "import re\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "from google.colab import files\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# KoNLPy 임포트\n",
        "from konlpy.tag import Okt, Mecab\n",
        "print(\"KoNLPy 임포트 성공\")\n",
        "\n",
        "# KoNLPy 형태소 분석기 초기화\n",
        "try:\n",
        "    # Mecab이 설치되어 있으면 사용 (더 빠름)\n",
        "    mecab = Mecab()\n",
        "    tokenizer = mecab\n",
        "    print(\"Mecab 형태소 분석기를 사용합니다.\")\n",
        "except Exception:\n",
        "    # 실패 시 Okt 사용 (기본)\n",
        "    okt = Okt()\n",
        "    tokenizer = okt\n",
        "    print(\"Okt 형태소 분석기를 사용합니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RDAlQZiIFyqY",
        "outputId": "47e9d66d-bf0a-4082-d1e0-b2d0ff187484"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KoNLPy 임포트 성공\n",
            "Okt 형태소 분석기를 사용합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 불용어 목록 (조사, 접속사, 일부 대명사 등)\n",
        "\n",
        "stopwords_df = pd.read_csv('/content/korean_stopwords.csv', encoding='utf-8')\n",
        "korean_stopwords = set(stopwords_df['stopwords'].tolist())\n",
        "\n",
        "# 자막 특화 추가 불용어\n",
        "subtitle_stopwords = {\n",
        "    '네', '예', '음', '어', '그래', '그렇군요', '네네', '으음', '으응', '응응', '어어',\n",
        "    '아아', '그러게', '그러네', '그쵸', '안녕', '안녕하세요', '잘가요', '잘가', '감사합니다',\n",
        "    '감사', '고마워', '네', '응', '어', '아니', '그래', '글쎄', '아휴', '아이고', '아이구',\n",
        "    '흠', '저', '다', '이런', '그런', '음', '하하', '호호', '아하', '우와', '우아', '와', '워',\n",
        "    '뭐', '왜', '어떻게', '무슨', '어디', '언제', '이제', '좀', '많이', '매우', '너무', '진짜',\n",
        "    '정말', '거의', '모두', '다', '약간', '조금', '거',\n",
        "    '같아요', '같아', '같은데', '같아서',\n",
        "    '그게', '이번', '하이', '반갑습니다', '반갑고',\n",
        "    '해야', '했고', '하면서', '되면', '있으면', '하겠습니다',\n",
        "    '계신가요', '주면', '될까', '아닙니다', '해보네요',\n",
        "    '제', '사실', '있으신', '하여튼', '해야지', '원래', '그래가지고', '있어가지고요', '그러면은', '어쨌든',\n",
        "    '하지만', '하겠어요', '걸', '갑자기', '그때', '누가', '있어요', '여기', '아니면','곧', '잠깐',\n",
        "    '있나요', '막', '그렇죠', '아까', '오케이', '했어', '하느라', '였나', '했어도',\n",
        "    '이었어요', '꼭', '있는', '겠다', '계셨나요', '아니어가지고', '와중에', '했네요','않겠다','왔던', '와중', '하고', '그냥', '였지',\n",
        "    '입니다', '아주', '게', '고요', '뭔가', '하는', '같은데', '있어', '됩니다', '있었어', '있어서',\n",
        "    '거기', '그거', '있습니다', '없어', '난', '같아', '그것'\n",
        "}\n",
        "\n",
        "# 모든 불용어 합치기\n",
        "all_stopwords = korean_stopwords.union(subtitle_stopwords)\n",
        "print(f\"정의된 불용어 수: {len(all_stopwords)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Fyr-kA1muc",
        "outputId": "bcddeaf9-ff27-44c2-c891-a7cfffe8ac20"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의된 불용어 수: 701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석 및 불용어 제거 함수 (복합 키워드 추출 기능 추가)\n",
        "def tokenize_and_filter(text, extract_compound=True):\n",
        "    \"\"\"KoNLPy를 이용한 형태소 분석 및 불용어 제거\"\"\"\n",
        "\n",
        "    # 1. 단일 토큰 추출\n",
        "    if isinstance(tokenizer, Okt):\n",
        "        # Okt 형태소 분석기 사용\n",
        "        pos_tagged = tokenizer.pos(text)\n",
        "    else:\n",
        "        # Mecab 형태소 분석기 사용\n",
        "        pos_tagged = tokenizer.pos(text)\n",
        "\n",
        "    # 단일 토큰 필터링 (명사, 형용사, 동사만 추출)\n",
        "    single_tokens = []\n",
        "    for word, pos in pos_tagged:\n",
        "        # 명사, 형용사, 동사 필터링 (형태소 분석기에 따라 태그가 다름)\n",
        "        if isinstance(tokenizer, Okt):\n",
        "            is_valid = pos in ['Noun', 'Adjective', 'Verb'] and len(word) > 1 and word not in all_stopwords\n",
        "        else:  # Mecab\n",
        "            is_valid = (pos.startswith('NN') or pos.startswith('VA') or pos.startswith('VV')) and len(word) > 1 and word not in all_stopwords\n",
        "\n",
        "        if is_valid:\n",
        "            single_tokens.append((word, pos))\n",
        "\n",
        "    # 복합 키워드 추출이 필요 없으면 단일 토큰만 반환\n",
        "    if not extract_compound:\n",
        "        return [word for word, _ in single_tokens]\n",
        "\n",
        "    # 2. 복합 키워드 추출 (연속된 토큰에서 특정 패턴 찾기)\n",
        "    compound_tokens = []\n",
        "\n",
        "    # 원본 텍스트에서 문장 단위로 처리 (문장 경계를 넘어 복합 키워드가 형성되지 않도록)\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # 문장별 형태소 분석\n",
        "        if isinstance(tokenizer, Okt):\n",
        "            sentence_pos = tokenizer.pos(sentence)\n",
        "        else:\n",
        "            sentence_pos = tokenizer.pos(sentence)\n",
        "\n",
        "        # 패턴 1: 형용사 + 명사 (예: \"새로운 기술\")\n",
        "        for i in range(len(sentence_pos)-1):\n",
        "            curr_word, curr_pos = sentence_pos[i]\n",
        "            next_word, next_pos = sentence_pos[i+1]\n",
        "\n",
        "            # Okt용 패턴\n",
        "            if isinstance(tokenizer, Okt):\n",
        "                # 형용사 + 명사\n",
        "                if curr_pos == 'Adjective' and next_pos == 'Noun':\n",
        "                    compound = curr_word + \" \" + next_word\n",
        "                    if len(compound) > 3 and not any(word in all_stopwords for word in [curr_word, next_word]):\n",
        "                        compound_tokens.append(compound)\n",
        "\n",
        "                # 명사 + 명사\n",
        "                if curr_pos == 'Noun' and next_pos == 'Noun':\n",
        "                    compound = curr_word + \" \" + next_word\n",
        "                    if len(compound) > 3 and not any(word in all_stopwords for word in [curr_word, next_word]):\n",
        "                        compound_tokens.append(compound)\n",
        "\n",
        "            # Mecab용 패턴\n",
        "            else:\n",
        "                # 형용사 + 명사\n",
        "                if curr_pos.startswith('VA') and next_pos.startswith('NN'):\n",
        "                    compound = curr_word + \" \" + next_word\n",
        "                    if len(compound) > 3 and not any(word in all_stopwords for word in [curr_word, next_word]):\n",
        "                        compound_tokens.append(compound)\n",
        "\n",
        "                # 명사 + 명사\n",
        "                if curr_pos.startswith('NN') and next_pos.startswith('NN'):\n",
        "                    compound = curr_word + \" \" + next_word\n",
        "                    if len(compound) > 3 and not any(word in all_stopwords for word in [curr_word, next_word]):\n",
        "                        compound_tokens.append(compound)\n",
        "\n",
        "        # 패턴 2: 동사 + 명사 (예: \"달리는 자동차\")\n",
        "        # for i in range(len(sentence_pos)-1):\n",
        "        #     curr_word, curr_pos = sentence_pos[i]\n",
        "        #     next_word, next_pos = sentence_pos[i+1]\n",
        "\n",
        "        #     # Okt용 패턴\n",
        "        #     if isinstance(tokenizer, Okt):\n",
        "        #         if curr_pos == 'Verb' and next_pos == 'Noun':\n",
        "        #             compound = curr_word + \" \" + next_word\n",
        "        #             if len(compound) > 4 and not any(word in all_stopwords for word in [curr_word, next_word]):\n",
        "        #                 compound_tokens.append(compound)\n",
        "\n",
        "        #     # Mecab용 패턴\n",
        "        #     else:\n",
        "        #         if curr_pos.startswith('VV') and next_pos.startswith('NN'):\n",
        "        #             compound = curr_word + \" \" + next_word\n",
        "        #             if len(compound) > 4 and not any(word in all_stopwords for word in [curr_word, next_word]):\n",
        "        #                 compound_tokens.append(compound)\n",
        "\n",
        "    # 3. 단일 토큰과 복합 토큰 결합 (단일 토큰은 단어만 추출)\n",
        "    all_tokens = [word for word, _ in single_tokens] + compound_tokens\n",
        "\n",
        "    return all_tokens"
      ],
      "metadata": {
        "id": "TWJs3zN7F7vo"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 파일 업로드 및 처리 코드 (이전과 동일)\n",
        "# print(\"VTT 자막 파일을 업로드하세요:\")\n",
        "# uploaded = files.upload()\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "file_name = '/content/0-156598630.vtt'\n",
        "\n",
        "try:\n",
        "    # 파일 읽기 (여러 인코딩 시도)\n",
        "    for encoding in ['utf-8', 'cp949', 'euc-kr']:\n",
        "        try:\n",
        "            with open(file_name, 'r', encoding=encoding) as file:\n",
        "                subtitle_text = file.read()\n",
        "                print(f\"파일을 {encoding} 인코딩으로 성공적으로 읽었습니다.\")\n",
        "                break\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "\n",
        "    # 전처리 적용\n",
        "    processed_text = preprocess_vtt(subtitle_text)\n",
        "    print(f\"전처리된 텍스트 샘플 (처음 300자): \\n{processed_text[:300]}...\\n\")\n",
        "\n",
        "    # 형태소 분석 및 토큰화 (복합 키워드 포함)\n",
        "    tokens = tokenize_and_filter(processed_text, extract_compound=True)\n",
        "\n",
        "    # 단일 키워드와 복합 키워드 구분해서 출력\n",
        "    single_tokens = [t for t in tokens if \" \" not in t]\n",
        "    compound_tokens = [t for t in tokens if \" \" in t]\n",
        "\n",
        "    print(f\"추출된 전체 토큰 수: {len(tokens)}\")\n",
        "    print(f\"단일 키워드 수: {len(single_tokens)}, 복합 키워드 수: {len(compound_tokens)}\")\n",
        "    # print(f\"단일 키워드 예시: {single_tokens[:10]}\")\n",
        "    # print(f\"복합 키워드 예시: {compound_tokens[:10]}\")\n",
        "    print(f\"단일 키워드 예시: {single_tokens}\")\n",
        "    print(f\"복합 키워드 예시: {compound_tokens}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"파일 처리 중 오류: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewz6n9y31xAp",
        "outputId": "896cbfe3-197c-48ac-ca4a-21655a0638eb"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파일을 utf-8 인코딩으로 성공적으로 읽었습니다.\n",
            "전처리된 텍스트 샘플 (처음 300자): \n",
            "안녕하세요 안녕 안녕 하이하이 다들 반갑고 다들 잘 지내고 계신가요 안녕 얼굴 보여 주면 안 될까 오늘 제가 아침에 피부과를 다녀왔어요 지금은 비출 수 있는 상태가 아닙니다 안녕 안녕 지금부터 ㅠㅠ 금지예요 반갑습니다 오랜만 이거 보이스 라이브 진짜 오랜만에 해보네요 엄청 뭐 바쁘게 살기도 했고 그다음에 바쁘게 살아 가지고 라이브 오는데 조금 쉽지 않았어요 ㅋㅋㅋ웃어야 겠다 좋아요 웃어주세요 캐럿들 잘 지내고 계셨나요 뭐 사실 투어 끝난 지 그렇게 엄청 오랜 시간이 지난 게 아니어가지고 그 와중에 저는 되게 준비도 많이 하고 여러 ...\n",
            "\n",
            "추출된 전체 토큰 수: 750\n",
            "단일 키워드 수: 635, 복합 키워드 수: 115\n",
            "단일 키워드 예시: ['지내고', '얼굴', '보여', '오늘', '아침', '피부', '다녀왔어요', '지금', '비출', '상태', '지금', '금지', '보이스', '라이브', '바쁘게', '살기', '그다음', '바쁘게', '가지', '라이브', '오는데', '쉽지', '않았어요', '웃어야', '좋아요', '웃어주세요', '캐럿', '지내고', '투어', '끝난', '지난', '준비', '여러', '버버리', '런던', '보고', '오고', '많은', '많은', '중간고사', '시즌', '쉽지', '생각', '이었는데요', '갑작스럽게', '미루어', '쏟아지면서', '스포', '생각', '얘기', '노래', '녹음', '라이브', '최대한', '보고', '빠른', '시일', '하지', '않을까', '싶어요', '런던', '처음', '갔다', '처음', '갔다', '글래스턴베리', '영국', '들리긴', '구경', '했어가지고', '갔는데', '구경', '왔어요', '오늘', '올린', '사진', '며칠', '지났지만', '녹음', '일도', '하다', '시차', '적응', '해가지고', '사진', '올려야', '되는데요', '피곤하다', '이러면서', '자고', '오늘', '올렸네요', '사진', '예전', '보정', '놓은', '뭉쳐가지고', '올렸습니다', '이쁜', '사진', '올려줘', '찾아볼게요', '감기', '걸렸어', '감기', '걸리면', '힘든데', '감기', '조심하세요', '저녁', '먹었냐', '저녁', '간단하게', '찌개', '먹었습니다', '컨디션', '괜찮냐', '시차', '적응', '했는지', '지났는데도', '패턴', '돌아오더라고요', '나름', '괜찮습니다', '고민', '되는', '게임', '될지', '하니까', '하게', '고민', '고민', '만이', '싶긴', '이즈', '같은', '이즈', '게임', '게임', '여럿', '하면', '재밌지', '않을까', '생각', '노래', '들을까', '예전', '앨범', '노래', '공연', '라이브', '듣는', '어른', '어른', '좋다', '어른', '바람개비', '좋았는데', '미안', '호시', '우지', '노래', '들어', '봤냐', '당연히', '들어', '봤죠', '들어', '봤고', '벌써', '멋있어', '호시', '우지', '주말', '주말', '주말', '주말', '힘들', '같은', '주말', '촬영', '스케줄', '주말', '같고', '중이', '않을가라는', '생각', '있긴', '중일', '린지', '준비', '중이', '린지', '호시', '우지', '동갑', '내기', '낸답니다', '런던', '갔을', '특별한', '있었냐', '특별했고', '처음', '겪는', '특별했지만', '이상할', '있는데', '특별하지', '않아서', '좋았던', '자유시간', '가지', '기도', '했는데', '평범하게', '보내', '좋더라고요', '특별한', '좋을', '경우', '있는데', '흘러가는', '일상', '좋은', '경우', '좋았던', '사진', '계정', '올렸던', '사진', '벤인가', '벤이', '맞을', '아마', '촬영', '스토리', '올렸던', '음악', '장소', '들으면서', '산책', '했었거든요', '좋아가지고', '나중', '사진', '사진', '찍어서', '보정', '해서', '올리면', '음악', '붙여서', '올려야지라는', '생각', '오늘', '올렸습니다', '흘러가는', '일상', '걸까', '특별한', '아니라', '건강하고', '아무', '흘려', '보내는', '하루', '평범한', '일상', '노래', '골라', '볼게요', '가요', '좋다', '패션', '크에서', '경험', '어땠나요', '좋았고', '재밌었어요', '경험', '보지', '했던', '경험', '재밌었고', '새로운', '방면', '같은', '느낌', '굉장히', '좋았습니다', '심심하냐고요', '경우', '없었어요', '회사', '보내가지고', '런던', '인상', '깊었던', '생각난', '산책', '계속', '했었거든요', '계속', '하다가', '가까이', '걷고', '돌아다니고', '하다가', '벤치', '벤치', '앉아서', '보냈다', '특별한', '기억', '남아있는', '계속', '걷고', '보내고', '달리다가', '벤치', '앉아서', '풍경', '구경', '쉬어', '기분', '좋았어', '여행', '묘미', '쉬는', '벤치', '앉아', '쉼이', '되고', '기억', '남더라고요', '좋다고', '느꼈어요', '여유', '느꼈던', '노래', '흥얼거리냐고요', '노래', '틀어', '놓은', '느끼면서', '어감', '어감', '값지게', '보내자', '생각', '노래', '좋아한다고요', '노래', '좋아요', '도겸', '노래', '좋아하는데', '보이스', '라이브', '와가지고', '어색하네', '음악', '틀어', '달라고요', '음악', '들을까', '한번', '캐럿', '나중', '놓을', '보고', '싶은', '게임', '올려주시면', '스태프', '고민', '해서', '한번', '골라', '보도록', '모르겠네', '공포', '게임', '공포', '게임', '단체', '게임', '싶긴', '이즈', '웃겼어가지고', '파티', '이서', '최대한', '좋은데', '경음악', '경음악', '재밌겠다', '경음악', '한번', '들을까', '경음악', '음악', '듣자', '신나게', '경음악', '신나는데', '음악', '듣고', '싶었어', '최애', '최애', '최애', '최애', '셔츠', '얇은', '목도리', '스카프', '같은', '해보고', '싶다라는', '생각', '해가지고', '아마', '도전', '않을까', '생각', '시험', '응원', '시험', '캐럿', '파이팅', '쿡스', '쿱스', '나가', '쿡스', '쿱스', '맞으려나', '확신', '맞을지', '맞을지', '몰라가지고', '생각', '멤버', '그렇고', '멤버', '그렇고', '그렇고', '보통', '공연', '콘서트', '팬미팅', '준비', '기간', '생각', '바빠가지고', '준비', '은근', '바쁘다', '되나', '커버', '올려', '달라', '커버', '커버', '올릴', '개인', '올릴', '예정', '어젯밤', '한번', '어젯밤', '어젯밤', '어젯밤', '휴지통', '휴지통', '휴지통', '휴지통', '좋아요', '휴지통', '틀어', '드릴게요', '며칠', '계속', '녹음', '나가가지고', '음악', '들으니까', '좋네', '아이콘', '비하인드', '풀어', '달라고요', '아이콘', '비하인드', '아이콘', '촬영', '돼가지고', '자세한', '내용', '기억', '나는데', '유쾌하게', '재밌게', '찍었던', '기억', '유쾌하게', '재밌게', '찍었습니다', '강아지', '기억난다', '강아지', '강아지', '기억', '나는데', '오늘', '오늘', '사진', '올렸던', '노래', '한번', '듣고', '오늘', '마무리', '해보도록', '할게요', '내일', '모르겠다', '녹음', '계속', '있어가지고', '자야되', '쉬어요', '캐럿', '쉬시길', '바랄게요', '내일', '공부', '캐럿', '많으니까', '바쁘잖아', '원우', '와서', '목소리', '자체', '위로', '되었다고요', '캐럿', '보면서', '힐링', '받습니다', '리듬', '탄다', '손톱', '건가', '듣는데', '좋네', '좋은데', '휴지통', '같은', '발매', '해줬으면', '좋겠어', '기다리십시오', '노래', '런던', '기억', '노래', '들으면', '벤을', '바라보며', '걷는', '느낌', '마지막', '자러', '가보도록', '캐럿', '오늘', '고생', '수고', '오늘', '하루', '주무시길', '바랄게요', '내일', '오늘', '힘들고', '행복한', '하루', '되셨으면', '좋겠어요', '보이스', '달라', '당연하죠', '하루', '마무리', '음악', '들으면서', '얘기', '나누고', '좋은', '올게요', '걱정', '하지', '지금', '런던', '있을', '감상', '빠져', '가는', '정도', '갔다', '왔나', '이틀', '정도', '완전', '날씨', '좋았어가지고', '날씨', '좋을', '걸었던', '기억', '노래', '들어가', '보도록', '캐럿', '오늘', '좋은', '되길', '바라요', '런던', '감상', '계속', '빠져있어']\n",
            "복합 키워드 예시: ['보이스 라이브', '바쁘게 살기', '바쁘게 살', '런던 쇼', '중간고사 시즌', '노래 녹음', '빠른 시일', '시일 내', '온 처음', '시차 적응', '이쁜 사진', '힘든데 감기', '간단하게 찌개', '시차 적응', '잠 패턴', '요 고민', '폴 이즈', '같은 폴', '폴 이즈', '이즈 게임', '예전 앨범', '좋다 어른', '미안 호시', '우지 노래', '멋있어 호시', '우지 주말', '촬영 스케줄', '주 중이', '주 중일', '린지 준비', '준비 중이', '동갑 내기', '이상할 수', '가지 기도', '좋을 경우', '좋은 경우', '사진 계정', '사진 빅', '건강하고 아무', '평범한 일상', '좋다 패션', '새로운 방면', '같은 느낌', '반 가까이', '특별한 기억', '풍경 구경', '좋았어 여행', '쉬 어감', '쉬 어감', '좋아요 도겸', '보이스 라이브', '어색하네 음악', '공포 게임', '공포 게임', '폴 이즈', '파티 애', '명 이서', '좋은데 경음악', '신 경음악', '재밌겠다 경음악', '신 한번', '봄 최애', '최애 템', '봄 최애', '최애 템', '봄 최애', '최애 템', '봄 최애', '최애 템', '얇은 목도리', '쿡스 쿱스', '쿡스 쿱스', '커버 곡', '커버 곡', '곡 커버', '커버 곡', '개인 곡', '한번 어젯밤', '어젯밤 어젯밤', '휴지통 휴지통', '휴지통 휴지통', '휴지통 휴지통', '요 며칠', '며칠 계속', '계속 녹음', '좋네 디', '디 아이콘', '아이콘 비하인드', '디 아이콘', '아이콘 비하인드', '디 아이콘', '아이콘 도', '도 촬영', '자세한 내용', '강아지 강아지', '강아지 기억', '오늘 오늘', '타 사진', '사진 인', '목소리 자체', '좋은데 휴지통', '같은 곡', '오늘 하루', '하루 푹', '힘들고 더', '행복한 하루', '좋겠어요 보이스', '하루 마무리', '이틀 정도', '완전 날씨', '기억 노래', '노래 끝', '좋은 밤', '런던 감상', '감상 계속']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF 기반 키워드 추출 함수 (복합 키워드 처리 추가)\n",
        "def extract_keywords(text, top_n=30):\n",
        "    \"\"\"TF-IDF와 추가 메트릭을 사용한 키워드 추출 (복합 키워드 포함)\"\"\"\n",
        "    # 복합 키워드를 포함한 토큰화\n",
        "    tokens = tokenize_and_filter(text, extract_compound=True)\n",
        "\n",
        "    # 빈도수 계산\n",
        "    word_freq = Counter(tokens)\n",
        "\n",
        "    # 단어가 너무 적으면 처리 중단\n",
        "    if len(word_freq) < 5:\n",
        "        print(\"추출된 키워드가 너무 적습니다.\")\n",
        "        return []\n",
        "\n",
        "    # 복합 키워드와 단일 키워드 구분\n",
        "    single_keywords = [word for word in word_freq.keys() if \" \" not in word]\n",
        "    compound_keywords = [word for word in word_freq.keys() if \" \" in word]\n",
        "\n",
        "    print(f\"고유 단일 키워드 수: {len(single_keywords)}\")\n",
        "    print(f\"고유 복합 키워드 수: {len(compound_keywords)}\")\n",
        "\n",
        "    # 문장 단위로 분할하여 TF-IDF 계산\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "    sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
        "\n",
        "    # 각 문장을 토큰화하여 문서로 변환\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # 문장별 토큰화 (복합 키워드 포함)\n",
        "        sentence_tokens = tokenize_and_filter(sentence, extract_compound=True)\n",
        "        # 토큰들을 공백으로 구분된 문자열로 변환\n",
        "        tokenized_sentences.append(' '.join(sentence_tokens))\n",
        "\n",
        "    # TF-IDF 적용\n",
        "    try:\n",
        "        # n-gram 범위를 1~2로 설정하여 단일 및 복합 키워드 모두 포착\n",
        "        tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(tokenized_sentences)\n",
        "\n",
        "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "        word_tfidf = {}\n",
        "\n",
        "        # 모든 토큰에 대해 TF-IDF 점수 계산\n",
        "        for word in word_freq.keys():\n",
        "            # 공백이 있는 복합 키워드는 TF-IDF 계산에서 제외될 수 있으므로 별도 처리\n",
        "            if \" \" in word:\n",
        "                # 복합 키워드의 구성 단어들의 TF-IDF 평균 사용\n",
        "                components = word.split()\n",
        "                component_scores = []\n",
        "                for component in components:\n",
        "                    if component in feature_names:\n",
        "                        word_idx = list(feature_names).index(component)\n",
        "                        component_scores.append(sum(tfidf_matrix[:, word_idx].toarray().flatten()))\n",
        "\n",
        "                # 구성 요소가 있으면 평균, 없으면 빈도 기반 점수\n",
        "                if component_scores:\n",
        "                    word_tfidf[word] = sum(component_scores) / len(component_scores)\n",
        "                    # 복합 키워드에 가중치 부여 (20% 증가)\n",
        "                    word_tfidf[word] *= 1.2\n",
        "                else:\n",
        "                    word_tfidf[word] = word_freq[word] / max(word_freq.values())\n",
        "            else:\n",
        "                # 단일 키워드는 기존 방식대로 TF-IDF 계산\n",
        "                if word in feature_names:\n",
        "                    word_idx = list(feature_names).index(word)\n",
        "                    word_tfidf[word] = sum(tfidf_matrix[:, word_idx].toarray().flatten())\n",
        "                else:\n",
        "                    word_tfidf[word] = 0\n",
        "    except Exception as e:\n",
        "        print(f\"TF-IDF 계산 중 오류: {e}\")\n",
        "        # 오류 발생 시 빈도수만 사용\n",
        "        word_tfidf = {word: word_freq[word] / max(word_freq.values()) for word in word_freq}\n",
        "\n",
        "    # 최종 가중치 계산 (TF-IDF + 빈도 + 단어 길이 + 복합 키워드 보너스)\n",
        "    keywords = {}\n",
        "    for word in word_freq.keys():\n",
        "        # 기본 가중치 = TF-IDF 점수\n",
        "        weight = word_tfidf.get(word, 0)\n",
        "\n",
        "        # 빈도 가중치 추가\n",
        "        freq_weight = min(word_freq[word] / max(word_freq.values()), 1.0)\n",
        "\n",
        "        # 단어 길이 가중치\n",
        "        length_weight = min(len(word) / 10, 1.0)\n",
        "\n",
        "        # 복합 키워드 보너스 (이미 TF-IDF에서 가중치를 줬지만 추가 보너스)\n",
        "        compound_bonus = 1.1 if \" \" in word else 1.0\n",
        "\n",
        "        # 최종 가중치\n",
        "        keywords[word] = (weight * 0.5 + freq_weight * 0.3 + length_weight * 0.2) * compound_bonus\n",
        "\n",
        "    # 상위 키워드 선택\n",
        "    top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    return top_keywords"
      ],
      "metadata": {
        "id": "dUeH4AtQGATp"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 통합 키워드 추출 및 랭킹 함수 (이전과 동일)\n",
        "def extract_and_rank_subtitle_keywords(text, top_n=100):\n",
        "    \"\"\"두 가지 방법을 조합한 키워드 추출 및 랭킹\"\"\"\n",
        "    # 두 가지 방법으로 키워드 추출\n",
        "    tfidf_keywords = extract_keywords(text, top_n)\n",
        "    print(f\"TF-IDF 키워드 추출 완료: {len(tfidf_keywords)} 개 추출됨\")\n",
        "\n",
        "    textrank_keywords_list = textrank_keywords(text, top_n)\n",
        "    print(f\"TextRank 키워드 추출 완료: {len(textrank_keywords_list)} 개 추출됨\")\n",
        "\n",
        "    # 결과 병합 및 가중치 조정\n",
        "    final_keywords = {}\n",
        "\n",
        "    print(f\"tfidf_keywords: {tfidf_keywords}\")\n",
        "    print(f\"textrank_keywords_list: {textrank_keywords_list}\")\n",
        "\n",
        "    # TF-IDF 결과 추가 (60% 가중치)\n",
        "    for word, weight in tfidf_keywords:\n",
        "        final_keywords[word] = weight * 0.6\n",
        "\n",
        "    # TextRank 결과 추가 (40% 가중치)\n",
        "    for word, weight in textrank_keywords_list:\n",
        "        if word in final_keywords:\n",
        "            final_keywords[word] += weight * 0.4\n",
        "        else:\n",
        "            final_keywords[word] = weight * 0.4\n",
        "\n",
        "    # 최종 결과 정렬\n",
        "    result = sorted(final_keywords.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    # 데이터프레임으로 변환\n",
        "    df = pd.DataFrame(result, columns=['Keyword', 'Weight'])\n",
        "\n",
        "    # 단일/복합 키워드 구분 컬럼 추가\n",
        "    df['Type'] = df['Keyword'].apply(lambda x: '복합 키워드' if ' ' in x else '단일 키워드')\n",
        "\n",
        "    # 가중치 정규화 (0-1 사이)\n",
        "    if not df.empty:\n",
        "        max_weight = df['Weight'].max()\n",
        "        df['Weight'] = df['Weight'] / max_weight\n",
        "\n",
        "    return df\n",
        "\n",
        "# 결과 시각화 및 저장 코드는 이전과 동일"
      ],
      "metadata": {
        "id": "cKi8KaOhGD-f"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메인 실행 코드\n",
        "try:\n",
        "    # processed_text 변수 확인\n",
        "    if 'processed_text' not in globals() or not processed_text:\n",
        "        raise ValueError(\"전처리된 텍스트를 찾을 수 없습니다. 이전 셀을 먼저 실행해주세요.\")\n",
        "\n",
        "    # 키워드 추출 실행\n",
        "    print(\"키워드 추출을 시작합니다...\")\n",
        "    keywords_df = extract_and_rank_subtitle_keywords(processed_text)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"\\n추출된 키워드 (상위 30개):\")\n",
        "\n",
        "    # 키워드 유형별 개수\n",
        "    single_count = len(keywords_df[keywords_df['Type'] == '단일 키워드'])\n",
        "    compound_count = len(keywords_df[keywords_df['Type'] == '복합 키워드'])\n",
        "    print(f\"단일 키워드: {single_count}개, 복합 키워드: {compound_count}개\")\n",
        "\n",
        "    # 모든 키워드 출력\n",
        "    print(keywords_df)\n",
        "\n",
        "    # 결과 CSV 파일로 저장\n",
        "    keywords_df.to_csv('subtitle_keywords.csv', index=False)\n",
        "    print(\"\\nCSV 파일로 결과가 저장되었습니다.\")\n",
        "    # files.download('subtitle_keywords.csv')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7PWmwy90Z_w",
        "outputId": "ba1cd0d4-c968-432e-e638-10e25c225551"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "키워드 추출을 시작합니다...\n",
            "고유 단일 키워드 수: 394\n",
            "고유 복합 키워드 수: 95\n",
            "TF-IDF 키워드 추출 완료: 100 개 추출됨\n",
            "TextRank 키워드 추출 완료: 100 개 추출됨\n",
            "tfidf_keywords: [('노래', np.float64(0.46905961780341376)), ('오늘', np.float64(0.3760939884502742)), ('휴지통 휴지통', np.float64(0.3477511213535402)), ('사진', np.float64(0.3332368455931314)), ('생각', np.float64(0.32598550195427745)), ('봄 최애', np.float64(0.2958581779527184)), ('최애 템', np.float64(0.2958581779527184)), ('아이콘 비하인드', np.float64(0.28466127496248445)), ('노래 끝', np.float64(0.2819301240719348)), ('휴지통', np.float64(0.2817811525405607)), ('음악', np.float64(0.2759583803302783)), ('캐럿', np.float64(0.268788401563422)), ('오늘 오늘', np.float64(0.2660726361829334)), ('기억 노래', np.float64(0.2660726361829334)), ('디 아이콘', np.float64(0.2658936334645388)), ('보이스 라이브', np.float64(0.26266127496248437)), ('게임', np.float64(0.26169978766856317)), ('기억', np.float64(0.26169978766856317)), ('좋은데 휴지통', np.float64(0.25801859033555646)), ('런던', np.float64(0.25452980890170684)), ('노래 녹음', np.float64(0.24714389223843272)), ('재밌겠다 경음악', np.float64(0.24689328843268032)), ('사진 빅', np.float64(0.2440726361829334)), ('타 사진', np.float64(0.2440726361829334)), ('사진 인', np.float64(0.2440726361829334)), ('어젯밤 어젯밤', np.float64(0.24382203237718097)), ('우지 노래', np.float64(0.2424117062523075)), ('오늘 하루', np.float64(0.23767952026618236)), ('좋겠어요 보이스', np.float64(0.23742891646042996)), ('좋은데 경음악', np.float64(0.2343576604049306)), ('강아지 강아지', np.float64(0.2343576604049306)), ('계속', np.float64(0.23310123747313546)), ('강아지 기억', np.float64(0.2312864043494313)), ('어색하네 음악', np.float64(0.22962547441880546)), ('특별한 기억', np.float64(0.22655421836330614)), ('폴 이즈', np.float64(0.22496488952003812)), ('커버 곡', np.float64(0.22496488952003812)), ('경음악', np.float64(0.2245026872777077)), ('공포 게임', np.float64(0.22339346094860954)), ('한번 어젯밤', np.float64(0.22182203237718098)), ('주말', np.float64(0.21876127993942282)), ('아이콘 도', np.float64(0.21875077632168166)), ('라이브', np.float64(0.21733270851085137)), ('이즈 게임', np.float64(0.21401859033555648)), ('최애', np.float64(0.21175403091656156)), ('이쁜 사진', np.float64(0.20928640434943133)), ('사진 계정', np.float64(0.20928640434943133)), ('신 경음악', np.float64(0.20928640434943133)), ('하루 마무리', np.float64(0.20762547441880547)), ('계속 녹음', np.float64(0.20455421836330615)), ('같은', np.float64(0.2045026872777077)), ('행복한 하루', np.float64(0.20289328843268029)), ('한번', np.float64(0.19733270851085138)), ('런던 쇼', np.float64(0.19675077632168167)), ('중간고사 시즌', np.float64(0.19650017251592924)), ('간단하게 찌개', np.float64(0.19650017251592924)), ('건강하고 아무', np.float64(0.19650017251592924)), ('어젯밤', np.float64(0.19590413708227994)), ('며칠 계속', np.float64(0.19508984639105584)), ('런던 감상', np.float64(0.19508984639105584)), ('감상 계속', np.float64(0.19508984639105584)), ('시차 적응', np.float64(0.1950003450318585)), ('쿡스 쿱스', np.float64(0.1950003450318585)), ('우지 주말', np.float64(0.19035766040493063)), ('아이콘', np.float64(0.18881552318742117)), ('멋있어 호시', np.float64(0.18869673047430477)), ('촬영 스케줄', np.float64(0.18869673047430477)), ('같은 폴', np.float64(0.1872864043494313)), ('같은 곡', np.float64(0.1872864043494313)), ('하루 푹', np.float64(0.1872864043494313)), ('같은 느낌', np.float64(0.18562547441880548)), ('바쁘게 살기', np.float64(0.1839645444881796)), ('힘든데 감기', np.float64(0.1839645444881796)), ('평범한 일상', np.float64(0.1839645444881796)), ('좋아요 도겸', np.float64(0.1839645444881796)), ('하루', np.float64(0.18307411584913627)), ('특별한', np.float64(0.1815641795485673)), ('준비 중이', np.float64(0.1808932884326803)), ('좋은 경우', np.float64(0.1808932884326803)), ('신 한번', np.float64(0.177822032377181)), ('곡 커버', np.float64(0.177822032377181)), ('린지 준비', np.float64(0.1761611024465551)), ('새로운 방면', np.float64(0.17450017251592928)), ('좋았어 여행', np.float64(0.17450017251592928)), ('얇은 목도리', np.float64(0.17450017251592928)), ('자세한 내용', np.float64(0.17450017251592928)), ('목소리 자체', np.float64(0.17450017251592928)), ('쉬 어감', np.float64(0.17300034503185852)), ('바쁘게 살', np.float64(0.17142891646042996)), ('좋다 어른', np.float64(0.17142891646042996)), ('좋을 경우', np.float64(0.17142891646042996)), ('준비', np.float64(0.16873415831542363)), ('녹음', np.float64(0.16873415831542363)), ('돌아오더라고요', np.float64(0.16859855019542774)), ('흥얼거리냐고요', np.float64(0.16859855019542774)), ('보이스', np.float64(0.1673055868868522)), ('강아지', np.float64(0.1673055868868522)), ('미안 호시', np.float64(0.16669673047430478)), ('좋다 패션', np.float64(0.1619645444881796)), ('풍경 구경', np.float64(0.1619645444881796))]\n",
            "textrank_keywords_list: [('노래', 0.014235262218778211), ('오늘', 0.011799822952061617), ('생각', 0.01115402730083293), ('캐럿', 0.009503302292467727), ('사진', 0.00949144849880403), ('음악', 0.008187902801814112), ('런던', 0.008184418913467124), ('기억', 0.007805725763717737), ('게임', 0.0071198496410553895), ('계속', 0.00661086432412833), ('같은', 0.0057701180303962405), ('라이브', 0.005620359399210221), ('한번', 0.004983417812701681), ('특별한', 0.004949785855862966), ('주말', 0.004919813020362509), ('준비', 0.004905024094800338), ('벤치', 0.0047479996901619494), ('하루', 0.004686033502239484), ('고민', 0.0045320478083548275), ('경음악', 0.00452642352674254), ('휴지통', 0.004517596588656081), ('녹음', 0.004487413826464304), ('폴 이즈', 0.0043716295321127524), ('경험', 0.004312680971230263), ('구경', 0.003979834839106258), ('감기', 0.003912581737596592), ('디 아이콘', 0.0037759852853725523), ('지금', 0.00373145525925076), ('경우', 0.0036774847820782654), ('갔다', 0.0036304974082226093), ('봄 최애', 0.003602080152725381), ('처음', 0.003579944260620253), ('보고', 0.0035661166465787843), ('내일', 0.0035634964397011767), ('최애 템', 0.0035520898315254245), ('보이스', 0.003540468176893294), ('호시', 0.003515686671994262), ('좋은', 0.0034837058922771677), ('않을까', 0.0034811863169834584), ('들어', 0.0034746697247063714), ('일상', 0.0034672196349104845), ('우지', 0.0034535930236763357), ('촬영', 0.003387510304282161), ('휴지통 휴지통', 0.0033757419605798915), ('최애', 0.0033741343479323496), ('시차 적응', 0.0033548165948557402), ('좋아요', 0.0033204012708664757), ('커버', 0.003287317173640864), ('올렸던', 0.0032587062691982267), ('커버 곡', 0.00321957069842036), ('이즈', 0.003209060739495684), ('틀어', 0.003202727784109384), ('아이콘', 0.003127319619899067), ('어른', 0.0030771875108319326), ('들을까', 0.0030645180332879167), ('어젯밤', 0.003022827862155617), ('보이스 라이브', 0.002976513392158935), ('그렇고', 0.0028866387294443916), ('저녁', 0.002877943500209348), ('강아지', 0.0028632494191784402), ('쉬 어감', 0.0028391451314647193), ('느낌', 0.002821906011860826), ('시차', 0.00280436581051998), ('적응', 0.0027878942319318313), ('정도', 0.0027543977097855407), ('가지', 0.002697431399194876), ('공포 게임', 0.002655885458599086), ('좋다', 0.0026554132631840394), ('아이콘 비하인드', 0.002616452819572812), ('있는데', 0.002613840178348499), ('쉽지', 0.002593140163990753), ('앉아서', 0.0025840039875368545), ('감상', 0.0025725544248139122), ('쿱스', 0.002570647408157796), ('좋았던', 0.0025631692111921667), ('쿡스', 0.002544799770722898), ('노래 끝', 0.002542113693133679), ('좋을', 0.002537566030811471), ('하지', 0.00250718576869852), ('걷고', 0.0025070172391916524), ('해가지고', 0.002506424334956565), ('날씨', 0.002484169777279684), ('달라', 0.0024809131832879564), ('하다가', 0.0024675765970394647), ('좋네', 0.002439949151096755), ('바쁘게', 0.0024376935717112308), ('바랄게요', 0.0024360097145495817), ('아마', 0.0024312471547882873), ('공연', 0.002418395021307402), ('산책', 0.0024090561403783566), ('중이', 0.002402531899106067), ('기억 노래', 0.0023982132513330427), ('올렸습니다', 0.002386554774877563), ('최대한', 0.0023699652135377386), ('흘러가는', 0.002364801401895418), ('놓은', 0.002359048184335075), ('골라', 0.0023548166328237427), ('싶긴', 0.0023546082262389544), ('했었거든요', 0.002352306463386064), ('좋은데', 0.0023521298825840017)]\n",
            "\n",
            "추출된 키워드 (상위 30개):\n",
            "단일 키워드: 27개, 복합 키워드: 73개\n",
            "    Keyword    Weight    Type\n",
            "0        노래  1.000000  단일 키워드\n",
            "1        오늘  0.802342  단일 키워드\n",
            "2   휴지통 휴지통  0.731380  복합 키워드\n",
            "3        사진  0.709570  단일 키워드\n",
            "4        생각  0.696733  단일 키워드\n",
            "..      ...       ...     ...\n",
            "95  돌아오더라고요  0.352311  단일 키워드\n",
            "96  흥얼거리냐고요  0.352311  단일 키워드\n",
            "97    미안 호시  0.348337  복합 키워드\n",
            "98    좋다 패션  0.338449  복합 키워드\n",
            "99    풍경 구경  0.338449  복합 키워드\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "\n",
            "CSV 파일로 결과가 저장되었습니다.\n"
          ]
        }
      ]
    }
  ]
}